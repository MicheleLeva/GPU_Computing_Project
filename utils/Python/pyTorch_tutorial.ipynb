{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"pyTorch_tutorial.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/b13fe25e4dd1e4b0fd5f5ce803bde74b/tensorqs_tutorial.ipynb","timestamp":1617879607584}]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ab98LCJxGWkG"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oRGUvEaaGWkM"},"source":["\n","`Learn the Basics <intro.html>`_ ||\n","`Quickstart <quickstart_tutorial.html>`_ || \n","**Tensors** || \n","`Datasets & DataLoaders <data_tutorial.html>`_ ||\n","`Transforms <transforms_tutorial.html>`_ ||\n","`Build Model <buildmodel_tutorial.html>`_ ||\n","`Autograd <autogradqs_tutorial.html>`_ ||\n","`Optimization <optimization_tutorial.html>`_ ||\n","`Save & Load Model <saveloadrun_tutorial.html>`_\n","\n","Tensors \n","==========================\n","\n","Tensors are a specialized data structure that are very similar to arrays and matrices. \n","In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n","\n","Tensors are similar to `NumPy’s <https://numpy.org/>`_ ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and\n","NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see `bridge-to-np-label`). Tensors \n","are also optimized for automatic differentiation (we'll see more about that later in the `Autograd <autogradqs_tutorial.html>`__ \n","section). If you’re familiar with ndarrays, you’ll be right at home with the Tensor API. If not, follow along!\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pt9I49sFyJi-","executionInfo":{"status":"ok","timestamp":1620206000798,"user_tz":-120,"elapsed":13873,"user":{"displayName":"Giuliano Grossi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5sNDnF8-qm3GijjkEscLaLhW25DgVgJ4Cf6SQ_Q=s64","userId":"05046904419500019805"}},"outputId":"ceb3956c-977e-4168-b02c-d4721b1f967c"},"source":["\n","!nvidia-smi\n","!pip install  cupy-cuda102"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Wed May  5 09:13:07 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Collecting cupy-cuda102\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/01/960de57c4894438946da7c28bea3d1a12ad088cb2d8b2c85703b51a1461a/cupy_cuda102-9.0.0-cp37-cp37m-manylinux1_x86_64.whl (60.5MB)\n","\u001b[K     |████████████████████████████████| 60.5MB 52kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda102) (1.19.5)\n","Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda102) (0.6)\n","Installing collected packages: cupy-cuda102\n","Successfully installed cupy-cuda102-9.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ibd4eOk-GWkN","executionInfo":{"status":"ok","timestamp":1620212627525,"user_tz":-120,"elapsed":64,"user":{"displayName":"Giuliano Grossi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5sNDnF8-qm3GijjkEscLaLhW25DgVgJ4Cf6SQ_Q=s64","userId":"05046904419500019805"}}},"source":["import torch\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5OOql-AGWkO"},"source":["Initializing a Tensor\n","~~~~~~~~~~~~~~~~~~~~~\n","\n","Tensors can be initialized in various ways. Take a look at the following examples:\n","\n","**Directly from data**\n","\n","Tensors can be created directly from data. The data type is automatically inferred.\n","\n"]},{"cell_type":"code","metadata":{"id":"E8Hkh3l1GWkO"},"source":["data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRH43kEKGWkO"},"source":["**From a NumPy array**\n","\n","Tensors can be created from NumPy arrays (and vice versa - see `bridge-to-np-label`).\n","\n"]},{"cell_type":"code","metadata":{"id":"eoC9y7rMGWkP"},"source":["np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A9Y1kP_UGWkP"},"source":["**From another tensor:**\n","\n","The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n","\n"]},{"cell_type":"code","metadata":{"id":"Ty9JBAlBGWkP"},"source":["x_ones = torch.ones_like(x_data) # retains the properties of x_data\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n","print(f\"Random Tensor: \\n {x_rand} \\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJfBJDrtGWkP"},"source":["**With random or constant values:**\n","\n","``shape`` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n","\n"]},{"cell_type":"code","metadata":{"id":"QezrxKPkGWkQ"},"source":["shape = (2,3,)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z_NkhCeAGWkQ"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ybRocaNfGWkQ"},"source":["Attributes of a Tensor\n","~~~~~~~~~~~~~~~~~\n","\n","Tensor attributes describe their shape, datatype, and the device on which they are stored.\n","\n"]},{"cell_type":"code","metadata":{"id":"Yc0sUL95GWkQ"},"source":["tensor = torch.rand(3,4)\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6ASE4vSGWkR"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"af5Bk9PNGWkR"},"source":["Operations on Tensors\n","~~~~~~~~~~~~~~~~~\n","\n","Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, \n","indexing, slicing), sampling and more are\n","comprehensively described `here <https://pytorch.org/docs/stable/torch.html>`__.\n","\n","Each of these operations can be run on the GPU (at typically higher speeds than on a\n","CPU). If you’re using Colab, allocate a GPU by going to Runtime > Change runtime type > GPU.\n","\n","By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using \n","``.to`` method (after checking for GPU availability). Keep in mind that copying large tensors\n","across devices can be expensive in terms of time and memory!\n","\n"]},{"cell_type":"code","metadata":{"id":"UcBRDdyEGWkR"},"source":["# We move our tensor to the GPU if available\n","if torch.cuda.is_available():\n","  tensor = tensor.to('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9MiYnuCGWkR"},"source":["Try out some of the operations from the list.\n","If you're familiar with the NumPy API, you'll find the Tensor API a breeze to use.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0Cs95BmOGWkS"},"source":["**Standard numpy-like indexing and slicing:**\n","\n"]},{"cell_type":"code","metadata":{"id":"QcZ6kUZhGWkS"},"source":["tensor = torch.ones(4, 4)\n","print('First row: ',tensor[0])\n","print('First column: ', tensor[:, 0])\n","print('Last column:', tensor[..., -1])\n","tensor[:,1] = 0\n","print(tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xoySjMg9GWkS"},"source":["**Joining tensors** You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension.\n","See also `torch.stack <https://pytorch.org/docs/stable/generated/torch.stack.html>`__,\n","another tensor joining op that is subtly different from ``torch.cat``.\n","\n"]},{"cell_type":"code","metadata":{"id":"OJNkEshaGWkS"},"source":["t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(t1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gWkaGav2GWkS"},"source":["**Arithmetic operations**\n","\n"]},{"cell_type":"code","metadata":{"id":"_dx_tb0YGWkT"},"source":["# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n","y1 = tensor @ tensor.T\n","y2 = tensor.matmul(tensor.T)\n","\n","y3 = torch.rand_like(tensor)\n","torch.matmul(tensor, tensor.T, out=y3)\n","\n","\n","# This computes the element-wise product. z1, z2, z3 will have the same value\n","z1 = tensor * tensor\n","z2 = tensor.mul(tensor)\n","\n","z3 = torch.rand_like(tensor)\n","torch.mul(tensor, tensor, out=z3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9swHGLKwGWkT"},"source":["**Single-element tensors** If you have a one-element tensor, for example by aggregating all\n","values of a tensor into one value, you can convert it to a Python\n","numerical value using ``item()``:\n","\n"]},{"cell_type":"code","metadata":{"id":"E_T05R0EGWkT"},"source":["agg = tensor.sum()\n","agg_item = agg.item()  \n","print(agg_item, type(agg_item))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RxegaK5IGWkT"},"source":["**In-place operations**\n","Operations that store the result into the operand are called in-place. They are denoted by a ``_`` suffix. \n","For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n","\n"]},{"cell_type":"code","metadata":{"id":"euYY8cf1GWkU"},"source":["print(tensor, \"\\n\")\n","tensor.add_(5)\n","print(tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRyVQ9AXGWkU"},"source":["<div class=\"alert alert-info\"><h4>Note</h4><p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss\n","     of history. Hence, their use is discouraged.</p></div>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZWYHFwgZGWkU"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yF6vwaGCGWkU"},"source":["\n","Bridge with NumPy\n","~~~~~~~~~~~~~~~~~\n","Tensors on the CPU and NumPy arrays can share their underlying memory\n","locations, and changing one will change\tthe other.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"02H75eubGWkU"},"source":["Tensor to NumPy array\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n"]},{"cell_type":"code","metadata":{"id":"5hD4a5BZGWkV"},"source":["t = torch.ones(5)\n","print(f\"t: {t}\")\n","n = t.numpy()\n","print(f\"n: {n}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U-XvzfMQGWkV"},"source":["A change in the tensor reflects in the NumPy array.\n","\n"]},{"cell_type":"code","metadata":{"id":"2kenRcf0GWkV"},"source":["t.add_(1)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8AJLMrTGWkV"},"source":["NumPy array to Tensor\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n"]},{"cell_type":"code","metadata":{"id":"eMRe6ottGWkV"},"source":["n = np.ones(5)\n","t = torch.from_numpy(n)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HW0ti0FtGWkV"},"source":["Changes in the NumPy array reflects in the tensor.\n","\n"]},{"cell_type":"code","metadata":{"id":"GwqFOe-GGWkW"},"source":["np.add(n, 1, out=n)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"],"execution_count":null,"outputs":[]}]}