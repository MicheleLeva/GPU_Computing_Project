{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BSP.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["F9PmBZql0ow4","8h2dznwofM8M","iUYP4kCJhEIx"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vjwQzqG-epop"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9PmBZql0ow4"},"source":["# CUDA setup"]},{"cell_type":"code","metadata":{"id":"p9RIwaPbVQHV"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5YlC1IOTlNb"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iVV0CidyVeqU"},"source":["## NVCC Plugin for Jupyter notebook\n","\n","*Usage*:\n","\n","\n","*   Load Extension `%load_ext nvcc_plugin`\n","*   Mark a cell to be treated as cuda cell\n","`%%cuda --name example.cu --compile false`\n","\n","**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n","\n","*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"X1EeyR1jBnWR"},"source":["!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7HcKDuAB-CO"},"source":["%load_ext nvcc_plugin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8h2dznwofM8M"},"source":["# CD"]},{"cell_type":"code","metadata":{"id":"qDzEZEUOfI_w"},"source":["%cd /content/drive/MyDrive/GPU_Computing_Project\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUYP4kCJhEIx"},"source":["#DeviceQuery"]},{"cell_type":"code","metadata":{"id":"qDVlDmZbirAW"},"source":["#@title working directory: **deviceQuery/**\n","%cd /content/drive/MyDrive/GPU_Computing_Project/deviceQuery\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPaV4DfPh26Q"},"source":["%%writefile ./helper.h\n","// Beginning of GPU Architecture definitions\n","inline int _ConvertSMVer2Cores(int major, int minor) {\n","\t// Defines for GPU Architecture types (using the SM version to determine\n","\t// the # of cores per SM\n","\ttypedef struct {\n","\t\tint SM;  // 0xMm (hexidecimal notation), M = SM Major version,\n","\t\t// and m = SM minor version\n","\t\tint Cores;\n","\t} sSMtoCores;\n","\n","\tsSMtoCores nGpuArchCoresPerSM[] = {\n","\t\t\t{0x20, 32},\n","\t\t\t{0x30, 192},\n","\t\t\t{0x32, 192},\n","\t\t\t{0x35, 192},\n","\t\t\t{0x37, 192},\n","\t\t\t{0x50, 128},\n","\t\t\t{0x52, 128},\n","\t\t\t{0x53, 128},\n","\t\t\t{0x60,  64},\n","\t\t\t{0x61, 128},\n","\t\t\t{0x62, 128},\n","\t\t\t{0x70,  64},\n","\t\t\t{0x72,  64},\n","\t\t\t{0x75,  64},\n","\t\t\t{-1, -1}};\n","\n","\tint index = 0;\n","\n","\twhile (nGpuArchCoresPerSM[index].SM != -1) {\n","\t\tif (nGpuArchCoresPerSM[index].SM == ((major << 4) + minor)) {\n","\t\t\treturn nGpuArchCoresPerSM[index].Cores;\n","\t\t}\n","\n","\t\tindex++;\n","\t}\n","\n","\t// If we don't find the values, we default use the previous one\n","\t// to run properly\n","\tprintf(\n","\t\t\t\"MapSMtoCores for SM %d.%d is undefined.\"\n","\t\t\t\"  Default to use %d Cores/SM\\n\",\n","\t\t\tmajor, minor, nGpuArchCoresPerSM[index - 1].Cores);\n","\treturn nGpuArchCoresPerSM[index - 1].Cores;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vvl8WXljg0WK"},"source":["%%writefile ./deviceQuery.cu\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"helper.h\"\n","#include \"../utils/common.h\"\n","\n","int main(void) {\n","\n","\tprintf(\"\\nCUDA Device Query (Runtime API) version (CUDART static linking)\\n\\n\");\n","\tint deviceCount = 0;\n","\tCHECK(cudaGetDeviceCount(&deviceCount));\n","\n","\t// This function call returns 0 if there are no CUDA capable devices.\n","\tif (deviceCount == 0)\n","\t\tprintf(\"There are no available device(s) that support CUDA\\n\");\n","\telse\n","\t\tprintf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n","\n","\tint dev, driverVersion = 0, runtimeVersion = 0;\n","\n","\tfor (dev = 0; dev < deviceCount; ++dev) {\n","\t\tcudaSetDevice(dev);\n","\t\tcudaDeviceProp deviceProp;\n","\t\tcudaGetDeviceProperties(&deviceProp, dev);\n","\n","\t\tprintf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n","\n","\t\tcudaDriverGetVersion(&driverVersion);\n","\t\tcudaRuntimeGetVersion(&runtimeVersion);\n","\n","\t\tprintf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n","\t\t\t\tdriverVersion / 1000, (driverVersion % 100) / 10,\n","\t\t\t\truntimeVersion / 1000, (runtimeVersion % 100) / 10);\n","\n","\t\tprintf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n","\t\t\t\tdeviceProp.major, deviceProp.minor);\n","\n","\t\tprintf(\"  Total amount of global memory:                 %.0f MBytes (%llu bytes)\\n\",\n","\t\t\t\t(float) deviceProp.totalGlobalMem / 1048576.0f,\n","\t\t\t\t(unsigned long long) deviceProp.totalGlobalMem);\n","\n","\t    printf(\"  (%2d) Multiprocessors, (%3d) CUDA Cores/MP:     %d CUDA Cores\\n\",\n","\t           deviceProp.multiProcessorCount,\n","\t           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor),\n","\t           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor) *\n","\t               deviceProp.multiProcessorCount);\n","\n","\t\tprintf(\"  GPU Max Clock rate:                            %.0f MHz (%0.2f GHz)\\n\",\n","\t\t\t\tdeviceProp.clockRate * 1e-3f, deviceProp.clockRate * 1e-6f);\n","\n","\t\tprintf(\"  Memory Clock rate:                             %.0f Mhz\\n\", deviceProp.memoryClockRate * 1e-3f);\n","\t\tprintf(\"  Memory Bus Width:                              %d-bit\\n\", deviceProp.memoryBusWidth);\n","\t\tif (deviceProp.l2CacheSize)\n","\t\t\tprintf(\"  L2 Cache Size:                                 %d bytes\\n\", deviceProp.l2CacheSize);\n","\n","\t\tprintf(\"  Maximum Texture Dimension Size (x,y,z)         1D=(%d), 2D=(%d, %d), 3D=(%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxTexture1D, deviceProp.maxTexture2D[0],\n","\t\t\t\tdeviceProp.maxTexture2D[1], deviceProp.maxTexture3D[0],\n","\t\t\t\tdeviceProp.maxTexture3D[1], deviceProp.maxTexture3D[2]);\n","\n","\t\tprintf(\"  Maximum Layered 1D Texture Size, (num) layers  1D=(%d), %d layers\\n\",\n","\t\t\t\tdeviceProp.maxTexture1DLayered[0],\n","\t\t\t\tdeviceProp.maxTexture1DLayered[1]);\n","\n","\t\tprintf(\"  Maximum Layered 2D Texture Size, (num) layers  2D=(%d, %d), %d layers\\n\",\n","\t\t\t\tdeviceProp.maxTexture2DLayered[0],\n","\t\t\t\tdeviceProp.maxTexture2DLayered[1],\n","\t\t\t\tdeviceProp.maxTexture2DLayered[2]);\n","\n","\t\tprintf(\"  Total amount of constant memory:               %lu bytes\\n\",\n","\t\t\t\tdeviceProp.totalConstMem);\n","\t\tprintf(\"  Total amount of shared memory per block:       %lu bytes\\n\",\n","\t\t\t\tdeviceProp.sharedMemPerBlock);\n","\t\tprintf(\"  Total number of registers available per block: %d\\n\",\n","\t\t\t\tdeviceProp.regsPerBlock);\n","\t\tprintf(\"  Warp size:                                     %d\\n\",\n","\t\t\t\tdeviceProp.warpSize);\n","\t\tprintf(\"  Maximum number of threads per multiprocessor:  %d\\n\",\n","\t\t\t\tdeviceProp.maxThreadsPerMultiProcessor);\n","\t\tprintf(\"  Maximum number of threads per block:           %d\\n\",\n","\t\t\t\tdeviceProp.maxThreadsPerBlock);\n","\t\tprintf(\"  Max dimension size of a thread block (x,y,z): (%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1],\n","\t\t\t\tdeviceProp.maxThreadsDim[2]);\n","\t\tprintf(\"  Max dimension size of a grid size    (x,y,z): (%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxGridSize[0], deviceProp.maxGridSize[1],\n","\t\t\t\tdeviceProp.maxGridSize[2]);\n","\t\tprintf(\"  Maximum memory pitch:                          %lu bytes\\n\",\n","\t\t\t\tdeviceProp.memPitch);\n","\t\tprintf(\"  Texture alignment:                             %lu bytes\\n\",\n","\t\t\t\tdeviceProp.textureAlignment);\n","\t\tprintf(\"  Concurrent copy and kernel execution:          %s with %d copy engine(s)\\n\",\n","\t\t\t\t(deviceProp.deviceOverlap ? \"Yes\" : \"No\"),\n","\t\t\t\tdeviceProp.asyncEngineCount);\n","\t\tprintf(\"  Run time limit on kernels:                     %s\\n\",\n","\t\t\t\tdeviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Integrated GPU sharing Host Memory:            %s\\n\",\n","\t\t\t\tdeviceProp.integrated ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Support host page-locked memory mapping:       %s\\n\",\n","\t\t\t\tdeviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Alignment requirement for Surfaces:            %s\\n\",\n","\t\t\t\tdeviceProp.surfaceAlignment ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Device has ECC support:                        %s\\n\",\n","\t\t\t\tdeviceProp.ECCEnabled ? \"Enabled\" : \"Disabled\");\n","\n","\t\tprintf(\"  Device supports Unified Addressing (UVA):      %s\\n\",\n","\t\t\t\tdeviceProp.unifiedAddressing ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Device PCI Domain ID / Bus ID / location ID:   %d / %d / %d\\n\",\n","\t\t\t\tdeviceProp.pciDomainID, deviceProp.pciBusID,\n","\t\t\t\tdeviceProp.pciDeviceID);\n","\t}\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kW9b_Yuxi7id"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc deviceQuery.cu -o deviceQuery\n","!./deviceQuery"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eA96rdPIi8lO"},"source":["# WarpSort"]},{"cell_type":"markdown","metadata":{"id":"Y_KtdBSNjJeU"},"source":["Qua ci va l'introduzione\n","\n","\n","*   In cosa consiste l'algoritmo \n","*   Come viene implementato\n","*   Caso sequenziale\n","*   Caso parallelo\n","*   Test e risultati\n","*   Conclusioni\n","\n"]},{"cell_type":"code","metadata":{"id":"h4pIymTnjH1T"},"source":["%cd /content/drive/MyDrive/GPU_Computing_Project/WarpSort/\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mqtxHaG-kHfR"},"source":["%%writefile ./WarpSort.cu\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include <time.h>\n","\n","#include \"../utils/common.h\"\n","\n","#define THREADS 16\n","#define BLOCKS 1\n","\n","\n","/*\n"," * Indici k e j in input hanno significato:\n"," *     k = 2,4,8,...,2^s=N\n"," *     j = 2^(k-1), 2^(k-2),...,1 (parte dalla metà di k e continua a dimezzare)\n"," * Gli operatori sui bit ^ (XOR) e & (AND) vengono usati per filtrare i thread:\n"," *     ixj = i ^ j  aggiunge o toglie a i una potenza di 2, cioé ixj = i +- j (j = 2^a)\n"," *     i & k == 0   vero sse i <= k (sort ascendente) altrimenti sort discendente\n"," * L'operazione ixj > i significa aggiorna solo quando l'indice ixj fa un salto in\n"," * avanti di j = 2^a\n"," * Funzionamento:\n"," */\n","__global__ void bitonic_sort_step(int *a, int j, int k) {\n","\tunsigned int i, ixj;                       // Sorting partners i and ixj\n","\ti = threadIdx.x + blockDim.x * blockIdx.x;\n","\tixj = i ^ j;   // XOR: aggiunge o toglie a i una potenza di 2, j = 2^a\n","\n","\tif (i == 0)\n","\t\tprintf(\"ROUND: k = %d, j = %d\\n\", k, j);\n","\n","\tif ((ixj) > i) {    // entra solo quando fa un salto di j = 2^a\n","\n","//\t\t Sort ascending\n","\t\tif ((i & k) == 0) {\n","\t\t\tprintf(\"  UP  (ixj = %d\\t    i = %d\\t k = %d)   a[ixj] = %d - a[i] = %d\\n\", ixj, i, k, a[ixj],a[i]);\n","\t\t\tif (a[i] > a[ixj]) {\n","\t\t\t\tint temp = a[i];\n","\t\t\t\ta[i] = a[ixj];\n","\t\t\t\ta[ixj] = temp;\n","\t\t\t}\n","\t\t}\n","\n","\t\t// Sort descending\n","\t\tif ((i & k) != 0) {\n","\t\t\tprintf(\"  DOWN  (ixj = %d\\t    i = %d\\t k = %d)   a[ixj] = %d - a[i] = %d\\n\", ixj, i, k, a[ixj],a[i]);\n","\t\t\tif (a[i] < a[ixj]) {\n","\t\t\t\tint temp = a[i];\n","\t\t\t\ta[i] = a[ixj];\n","\t\t\t\ta[ixj] = temp;\n","\t\t\t}\n","\t\t}\n","\t}\n","}\n","\n","__global__ void bitonic_sort_warp(int *keyin){\n","  //prendere thread id giusto tenendo in considerazione k_0 e k_1\n","  //implementare gli swap fatti bene dentro la funzione\n","\n","  int i,j,k_0,k_1 = 0;\n","  //phase 0 to log(128)-1 \n","  for(i=2;i<128;i*=2){ \n","    for(j=i/2;j>0;j/=2){ \n","      //k_0 ? position of preceding element in each pair to form ascending order\n","      if(keyin[k_0]>keyin[k_0+j]) \n","        swap(keyin[k_0],keyin[k_0+j]);\n","      //k1 ? position of preceding element in each pair to form descending order\n","      if(keyin[k_1]<keyin[k_1+j]) \n","        swap(keyin[k_1],keyin[k_1+j]);\n","    }\n","  }\n","\n","  //special case for the last phase \n","  for(j=128/2;j>0;j/=2){ \n","    //k0 ? position of preceding element in the thread's first pair to form ascending order\n","    if(keyin[k_0]>keyin[k_0+j]) \n","      swap(keyin[k_0],keyin[k_0+j]);\n","    //k1 ? position of preceding element in the thread's second pair to form ascending order\n","    if(keyin[k_1]>keyin[k_1+j]) \n","      swap(keyin[k_1],keyin[k_1+j]);\n","  }\n","}\n","\n","/*The parameter dir indicates the sorting direction, ASCENDING\n"," or DESCENDING; if (a[i] > a[j]) agrees with the direction,\n"," then a[i] and a[j] are interchanged.*/\n","void compAndSwap(int a[], int i, int j, int dir) {\n","\tif (dir == (a[i] > a[j])) {\n","\t\tint tmp = a[i];\n","\t\ta[i] = a[j];\n","\t\ta[j] = tmp;\n","\t}\n","}\n","\n","/*It recursively sorts a bitonic sequence in ascending order,\n"," if dir = 1, and in descending order otherwise (means dir=0).\n"," The sequence to be sorted starts at index position low,\n"," the parameter cnt is the number of elements to be sorted.*/\n","void bitonicMerge(int a[], int low, int cnt, int dir) {\n","\tif (cnt > 1) {\n","\t\tint k = cnt / 2;\n","\t\tfor (int i = low; i < low + k; i++)\n","\t\t\tcompAndSwap(a, i, i + k, dir);\n","\t\tbitonicMerge(a, low, k, dir);\n","\t\tbitonicMerge(a, low + k, k, dir);\n","\t}\n","}\n","\n","/* This function first produces a bitonic sequence by recursively\n"," sorting its two halves in opposite sorting orders, and then\n"," calls bitonicMerge to make them in the same order */\n","void bitonicSort(int a[], int low, int cnt, int dir) {\n","\tif (cnt > 1) {\n","\t\tint k = cnt / 2;\n","\n","\t\t// sort in ascending order since dir here is 1\n","\t\tbitonicSort(a, low, k, 1);\n","\n","\t\t// sort in descending order since dir here is 0\n","\t\tbitonicSort(a, low + k, k, 0);\n","\n","\t\t// Will merge wole sequence in ascending order\n","\t\t// since dir=1.\n","\t\tbitonicMerge(a, low, cnt, dir);\n","\t}\n","}\n","\n","/*\n"," * test bitonic sort on CPU and GPU\n"," */\n","int main(void) {\n","\tcudaEvent_t start, stop;\n","\tcudaEventCreate(&start);\n","\tcudaEventCreate(&stop);\n","\n","\tint N = THREADS*BLOCKS;\n","\t// check\n","\tif (!(N && !(N & (N - 1)))) {\n","\t\tprintf(\"ERROR: N must be power of 2 (N = %d)\\n\", N);\n","\t\texit(1);\n","\t}\n","\tsize_t nBytes = N * sizeof(int);\n","\tint *a = (int*) malloc(nBytes);\n","\tint *b = (int*) malloc(nBytes);\n","\n","\t// fill data\n","\tfor (int i = 0; i < N; ++i) {\n","\t\ta[i] =  i%5; //rand() % 100; // / (float) RAND_MAX;\n","\t\tb[i] = a[i];\n","\t}\n","\n","\t// bitonic CPU\n","\tdouble cpu_time = seconds();\n","\tbitonicSort(b, 0, N, 1);   // 1 means sort in ascending order\n","\tprintf(\"CPU elapsed time: %.5f (sec)\\n\", seconds()-cpu_time);\n","\n","\t// device mem copy\n","\tint *d_a;\n","\tCHECK(cudaMalloc((void**) &d_a, nBytes));\n","\tCHECK(cudaMemcpy(d_a, a, nBytes, cudaMemcpyHostToDevice));\n","\n","\t// num of threads\n","\tdim3 blocks(BLOCKS, 1);   // Number of blocks\n","\tdim3 threads(THREADS, 1); // Number of threads\n","\n","\t// start computation\n","\tcudaEventRecord(start);\n","\tint j, k;\n","\t// external loop on comparators of size k\n","\tfor (k = 2; k <= N; k <<= 1) {\n","\t\t// internal loop for comparator internal stages\n","\t\tfor (j = k >> 1; j > 0; j = j >> 1)\n","\t\t\tbitonic_sort_step<<<blocks, threads>>>(d_a, j, k);\n","\t}\n","\n","  /*Divide the input sequence into equal-sized subsequences. \n","  Each subsequence will be sorted by an independent warp using the bitonic network.*/\n","  bitonic_sort_warp<<<blocks, threads>>>(d_a);\n","\n","\tcudaEventRecord(stop);\n","\tcudaEventSynchronize(stop);\n","\tfloat milliseconds = 0;\n","\tcudaEventElapsedTime(&milliseconds, start, stop);\n","\tprintf(\"GPU elapsed time: %.5f (sec)\\n\", milliseconds / 1000);\n","\n","\t// recover data\n","\tcudaMemcpy(a, d_a, nBytes, cudaMemcpyDeviceToHost);\n","\n","\t// print & check\n","\tif (N < 100) {\n","\t\tprintf(\"GPU:\\n\");\n","\t\tfor (int i = 0; i < N; ++i)\n","\t\t\tprintf(\"%d\\n\", a[i]);\n","\t\tprintf(\"CPU:\\n\");\n","\t\tfor (int i = 0; i < N; ++i)\n","\t\t\tprintf(\"%d\\n\", b[i]);\n","\t}\n","\telse {\n","\t\tfor (int i = 0; i < N; ++i) {\n","\t\t\tif (a[i] != b[i]) {\n","\t\t\t\tprintf(\"ERROR a[%d] != b[%d]  (a[i] = %d  -  b[i] = %d\\n\", i,i, a[i],b[i]);\n","\t\t\t\tbreak;\n","\t\t\t}\n","\t\t}\n","\t}\n","\n","\tcudaFree(d_a);\n","\texit(0);\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-wjq_suoB0z"},"source":["# compilatore\n","\n","!nvcc -arch=sm_37 WarpSort.cu -o WarpSort\n","# !./WarpSort"],"execution_count":null,"outputs":[]}]}