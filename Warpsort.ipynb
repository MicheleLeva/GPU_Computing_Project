{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Warpsort.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["F9PmBZql0ow4","8h2dznwofM8M"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vjwQzqG-epop"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9PmBZql0ow4"},"source":["# CUDA setup"]},{"cell_type":"code","metadata":{"id":"p9RIwaPbVQHV"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5YlC1IOTlNb"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iVV0CidyVeqU"},"source":["## NVCC Plugin for Jupyter notebook\n","\n","*Usage*:\n","\n","\n","*   Load Extension `%load_ext nvcc_plugin`\n","*   Mark a cell to be treated as cuda cell\n","`%%cuda --name example.cu --compile false`\n","\n","**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n","\n","*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"X1EeyR1jBnWR"},"source":["!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7HcKDuAB-CO"},"source":["%load_ext nvcc_plugin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8h2dznwofM8M"},"source":["# CD"]},{"cell_type":"code","metadata":{"id":"qDzEZEUOfI_w"},"source":["%cd /content/drive/MyDrive/GPU_Computing_Project\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7yZYCuVxpngN"},"source":["# âœ” VS Code on Colab"]},{"cell_type":"code","metadata":{"id":"DKHvaMw3puK7"},"source":["# 1. Install the colab-code package...\n","!pip install colabcode"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YIGztaKepvqO"},"source":["# 2. Import and launch...\n","from colabcode import ColabCode\n","ColabCode()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUYP4kCJhEIx"},"source":["#DeviceQuery"]},{"cell_type":"code","metadata":{"id":"qDVlDmZbirAW"},"source":["#@title working directory: **deviceQuery/**\n","%cd /content/drive/MyDrive/GPU_Computing_Project/deviceQuery\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPaV4DfPh26Q"},"source":["%%writefile ./helper.h\n","// Beginning of GPU Architecture definitions\n","inline int _ConvertSMVer2Cores(int major, int minor) {\n","\t// Defines for GPU Architecture types (using the SM version to determine\n","\t// the # of cores per SM\n","\ttypedef struct {\n","\t\tint SM;  // 0xMm (hexidecimal notation), M = SM Major version,\n","\t\t// and m = SM minor version\n","\t\tint Cores;\n","\t} sSMtoCores;\n","\n","\tsSMtoCores nGpuArchCoresPerSM[] = {\n","\t\t\t{0x20, 32},\n","\t\t\t{0x30, 192},\n","\t\t\t{0x32, 192},\n","\t\t\t{0x35, 192},\n","\t\t\t{0x37, 192},\n","\t\t\t{0x50, 128},\n","\t\t\t{0x52, 128},\n","\t\t\t{0x53, 128},\n","\t\t\t{0x60,  64},\n","\t\t\t{0x61, 128},\n","\t\t\t{0x62, 128},\n","\t\t\t{0x70,  64},\n","\t\t\t{0x72,  64},\n","\t\t\t{0x75,  64},\n","\t\t\t{-1, -1}};\n","\n","\tint index = 0;\n","\n","\twhile (nGpuArchCoresPerSM[index].SM != -1) {\n","\t\tif (nGpuArchCoresPerSM[index].SM == ((major << 4) + minor)) {\n","\t\t\treturn nGpuArchCoresPerSM[index].Cores;\n","\t\t}\n","\n","\t\tindex++;\n","\t}\n","\n","\t// If we don't find the values, we default use the previous one\n","\t// to run properly\n","\tprintf(\n","\t\t\t\"MapSMtoCores for SM %d.%d is undefined.\"\n","\t\t\t\"  Default to use %d Cores/SM\\n\",\n","\t\t\tmajor, minor, nGpuArchCoresPerSM[index - 1].Cores);\n","\treturn nGpuArchCoresPerSM[index - 1].Cores;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vvl8WXljg0WK"},"source":["%%writefile ./deviceQuery.cu\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"helper.h\"\n","#include \"../utils/common.h\"\n","\n","int main(void) {\n","\n","\tprintf(\"\\nCUDA Device Query (Runtime API) version (CUDART static linking)\\n\\n\");\n","\tint deviceCount = 0;\n","\tCHECK(cudaGetDeviceCount(&deviceCount));\n","\n","\t// This function call returns 0 if there are no CUDA capable devices.\n","\tif (deviceCount == 0)\n","\t\tprintf(\"There are no available device(s) that support CUDA\\n\");\n","\telse\n","\t\tprintf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n","\n","\tint dev, driverVersion = 0, runtimeVersion = 0;\n","\n","\tfor (dev = 0; dev < deviceCount; ++dev) {\n","\t\tcudaSetDevice(dev);\n","\t\tcudaDeviceProp deviceProp;\n","\t\tcudaGetDeviceProperties(&deviceProp, dev);\n","\n","\t\tprintf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n","\n","\t\tcudaDriverGetVersion(&driverVersion);\n","\t\tcudaRuntimeGetVersion(&runtimeVersion);\n","\n","\t\tprintf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n","\t\t\t\tdriverVersion / 1000, (driverVersion % 100) / 10,\n","\t\t\t\truntimeVersion / 1000, (runtimeVersion % 100) / 10);\n","\n","\t\tprintf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n","\t\t\t\tdeviceProp.major, deviceProp.minor);\n","\n","\t\tprintf(\"  Total amount of global memory:                 %.0f MBytes (%llu bytes)\\n\",\n","\t\t\t\t(float) deviceProp.totalGlobalMem / 1048576.0f,\n","\t\t\t\t(unsigned long long) deviceProp.totalGlobalMem);\n","\n","\t    printf(\"  (%2d) Multiprocessors, (%3d) CUDA Cores/MP:     %d CUDA Cores\\n\",\n","\t           deviceProp.multiProcessorCount,\n","\t           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor),\n","\t           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor) *\n","\t               deviceProp.multiProcessorCount);\n","\n","\t\tprintf(\"  GPU Max Clock rate:                            %.0f MHz (%0.2f GHz)\\n\",\n","\t\t\t\tdeviceProp.clockRate * 1e-3f, deviceProp.clockRate * 1e-6f);\n","\n","\t\tprintf(\"  Memory Clock rate:                             %.0f Mhz\\n\", deviceProp.memoryClockRate * 1e-3f);\n","\t\tprintf(\"  Memory Bus Width:                              %d-bit\\n\", deviceProp.memoryBusWidth);\n","\t\tif (deviceProp.l2CacheSize)\n","\t\t\tprintf(\"  L2 Cache Size:                                 %d bytes\\n\", deviceProp.l2CacheSize);\n","\n","\t\tprintf(\"  Maximum Texture Dimension Size (x,y,z)         1D=(%d), 2D=(%d, %d), 3D=(%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxTexture1D, deviceProp.maxTexture2D[0],\n","\t\t\t\tdeviceProp.maxTexture2D[1], deviceProp.maxTexture3D[0],\n","\t\t\t\tdeviceProp.maxTexture3D[1], deviceProp.maxTexture3D[2]);\n","\n","\t\tprintf(\"  Maximum Layered 1D Texture Size, (num) layers  1D=(%d), %d layers\\n\",\n","\t\t\t\tdeviceProp.maxTexture1DLayered[0],\n","\t\t\t\tdeviceProp.maxTexture1DLayered[1]);\n","\n","\t\tprintf(\"  Maximum Layered 2D Texture Size, (num) layers  2D=(%d, %d), %d layers\\n\",\n","\t\t\t\tdeviceProp.maxTexture2DLayered[0],\n","\t\t\t\tdeviceProp.maxTexture2DLayered[1],\n","\t\t\t\tdeviceProp.maxTexture2DLayered[2]);\n","\n","\t\tprintf(\"  Total amount of constant memory:               %lu bytes\\n\",\n","\t\t\t\tdeviceProp.totalConstMem);\n","\t\tprintf(\"  Total amount of shared memory per block:       %lu bytes\\n\",\n","\t\t\t\tdeviceProp.sharedMemPerBlock);\n","\t\tprintf(\"  Total number of registers available per block: %d\\n\",\n","\t\t\t\tdeviceProp.regsPerBlock);\n","\t\tprintf(\"  Warp size:                                     %d\\n\",\n","\t\t\t\tdeviceProp.warpSize);\n","\t\tprintf(\"  Maximum number of threads per multiprocessor:  %d\\n\",\n","\t\t\t\tdeviceProp.maxThreadsPerMultiProcessor);\n","\t\tprintf(\"  Maximum number of threads per block:           %d\\n\",\n","\t\t\t\tdeviceProp.maxThreadsPerBlock);\n","\t\tprintf(\"  Max dimension size of a thread block (x,y,z): (%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1],\n","\t\t\t\tdeviceProp.maxThreadsDim[2]);\n","\t\tprintf(\"  Max dimension size of a grid size    (x,y,z): (%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxGridSize[0], deviceProp.maxGridSize[1],\n","\t\t\t\tdeviceProp.maxGridSize[2]);\n","\t\tprintf(\"  Maximum memory pitch:                          %lu bytes\\n\",\n","\t\t\t\tdeviceProp.memPitch);\n","\t\tprintf(\"  Texture alignment:                             %lu bytes\\n\",\n","\t\t\t\tdeviceProp.textureAlignment);\n","\t\tprintf(\"  Concurrent copy and kernel execution:          %s with %d copy engine(s)\\n\",\n","\t\t\t\t(deviceProp.deviceOverlap ? \"Yes\" : \"No\"),\n","\t\t\t\tdeviceProp.asyncEngineCount);\n","\t\tprintf(\"  Run time limit on kernels:                     %s\\n\",\n","\t\t\t\tdeviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Integrated GPU sharing Host Memory:            %s\\n\",\n","\t\t\t\tdeviceProp.integrated ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Support host page-locked memory mapping:       %s\\n\",\n","\t\t\t\tdeviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Alignment requirement for Surfaces:            %s\\n\",\n","\t\t\t\tdeviceProp.surfaceAlignment ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Device has ECC support:                        %s\\n\",\n","\t\t\t\tdeviceProp.ECCEnabled ? \"Enabled\" : \"Disabled\");\n","\n","\t\tprintf(\"  Device supports Unified Addressing (UVA):      %s\\n\",\n","\t\t\t\tdeviceProp.unifiedAddressing ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Device PCI Domain ID / Bus ID / location ID:   %d / %d / %d\\n\",\n","\t\t\t\tdeviceProp.pciDomainID, deviceProp.pciBusID,\n","\t\t\t\tdeviceProp.pciDeviceID);\n","\t}\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kW9b_Yuxi7id"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc deviceQuery.cu -o deviceQuery\n","!./deviceQuery"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eA96rdPIi8lO"},"source":["# WarpSort"]},{"cell_type":"markdown","metadata":{"id":"Y_KtdBSNjJeU"},"source":["Qua ci va l'introduzione\n","\n","\n","*   In cosa consiste l'algoritmo \n","*   Come viene implementato\n","*   Caso sequenziale\n","*   Caso parallelo\n","*   Test e risultati\n","*   Conclusioni\n","\n"]},{"cell_type":"code","metadata":{"id":"h4pIymTnjH1T"},"source":["%cd /content/drive/MyDrive/GPU_Computing_Project/WarpSort/\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mqtxHaG-kHfR"},"source":["%%writefile ./WarpSort.cu\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include <time.h>\n","#include <math.h>\n","\n","#include \"../utils/common.h\"\n","\n","#define THREADS 32\n","#define BLOCKS 32\n","#define T 64\n","#define K 8\n","\n","\n","int * get_splitters (int * input, int N, int s); //preliminary\n","\n","__global__ void bitonic_sort_warp(int *keyin); //step1\n","\n","__global__ void bitonic_warp_merge(int * keyin, int * output, int offset); //step2\n","\n","__global__ void print_array_kernel(int * input, int length);\n","\n","int get_length (int * array);\n","\n","/******FUNCTIONS*****/\n","\n","//kernel che stampa i contenuti dell'array in input\n","__global__ void print_array_kernel(int * input, int length){\n","    for(int i = 0; i < length; i++){\n","        printf(\"Array[%d] = %d \\n\", i, input[i]);\n","    }\n","}\n","\n","//Preliminary splitter preparation function\n","int * get_splitters (int * input, int N, int s){\n","  int numElements = s * K;\n","\n","  //printf(\"numElements = %d\\n\", numElements);\n","\n","  size_t nBytes = numElements * sizeof(int);\n","\tint *arrayA = (int*) malloc(nBytes);\n","\n","  //seleziona a random numElements elementi da input\n","  for (int i = 0; i < numElements; i++){\n","      arrayA[i] = input[rand() % numElements];\n","      //printf(\"arrayA[%d] = %d\\n\", i, arrayA[i]);\n","  }\n","\n","  // num of threads\n","\tdim3 blocks(BLOCKS, 1);   // Number of blocks\n","  dim3 threads(THREADS, 1); // Number of threads\n","\n","  //device memcopy\n","  int *bufferA, *bufferB;\n","  CHECK(cudaMalloc((void**) &bufferA, nBytes));\n","  CHECK(cudaMalloc((void**) &bufferB, nBytes));\n","\tCHECK(cudaMemcpy(bufferA, arrayA, nBytes, cudaMemcpyHostToDevice));\n","  free(arrayA);\n","\n","  //printf(\"\\nbufferA prima step1 SORT\\n\\n\");\n","  //print_array_kernel<<<1, 1>>>(bufferA, numElements);\n","\n","  //sorting degli elementi\n","\n","  bitonic_sort_warp<<<blocks, threads>>>(bufferA);\n","\n","  //printf(\"\\nbufferA dopo step1 SORT\\n\\n\");\n","  //print_array_kernel<<<1, 1>>>(bufferA, numElements);\n","\n","  bool isAfirst = true;\n","  if(numElements > 128){\n","    //ad ogni warp merge si inverte input ed output\n","    \n","    blocks.x = BLOCKS / 2;   // Number of blocks\n","    for(int offset = THREADS * 8; N / offset > 1; offset *= 2){\n","      //printf(\"N = %d, offset = %d, blocks.x = %d, threads.x = %d\\n\", N, offset, blocks, threads);\n","      if(isAfirst)\n","        bitonic_warp_merge<<<blocks, threads>>>(bufferA, bufferB, offset);\n","      else\n","        bitonic_warp_merge<<<blocks, threads>>>(bufferB, bufferA, offset);\n","      blocks.x = blocks.x / 2;\n","      \n","      isAfirst = !isAfirst;\n","    }\n","  }\n","  \n","  //printf(\"\\nbufferA dopo SORT\\n\\n\");\n","  //print_array_kernel<<<1, 1>>>(bufferA, numElements);\n","\n","  //printf(\"\\nbufferB dopo SORT\\n\\n\");\n","  //print_array_kernel<<<1, 1>>>(bufferB, numElements);\n","\n","  int * orderedSequence = (int*) malloc(numElements * sizeof(int));\n","  if(isAfirst){\n","    cudaMemcpy(orderedSequence, bufferA, nBytes, cudaMemcpyDeviceToHost);\n","  } else {\n","    cudaMemcpy(orderedSequence, bufferB, nBytes, cudaMemcpyDeviceToHost);\n","  }\n","\n","  cudaFree(bufferA);\n","  cudaFree(bufferB);\n","\n","  /*\n","  for (int i = 0; i < numElements; i++){\n","      printf(\"orderedSequence[%d] = %d\\n\", i, orderedSequence[i]);\n","  }*/\n","\n","  //seleziona k elementi dal buffer ordinato e restituisci\n","  int *output = (int*) malloc(s*sizeof(int));\n","  int last_split = -1;\n","  for(int i = 0; i < s; i ++){\n","      output[i] = orderedSequence[i*K];\n","      if (output[i] == last_split){\n","          output[i]++;\n","      }\n","      last_split = output[i];\n","      printf(\"output[%d] = %d\\n\", i, output[i]);\n","  }\n","\n","  free(orderedSequence);\n","   \n","  return output;\n","\n","}\n","\n","/*STEP 1:  Divide the input sequence into equal-sized subsequences. \n","  Each subsequence will be sorted by an independent warp using the bitonic network.*/\n","__global__ void bitonic_sort_warp(int *keyin){\n","  //prendere thread id giusto tenendo in considerazione k_0 e k_1\n","  //implementare gli swap fatti bene dentro la funzione\n","  unsigned int id = threadIdx.x + blockDim.x * blockIdx.x;\n","  unsigned int subseq = id / 32; //in quale sottosequenza dell'array siamo\n","  unsigned int start = 128 * subseq; //primo elemento della sottosequenza da riordinare\n","\n","  int i = 0, j = 0;\n","  int phase = 0, stage = 0;\n","  int k_0 = 0, k_1 = 0;\n","  int u = 0, index1 = 0, index2 = 0, p = 0, q = 0, m = 0, o = 0, um = 0, pm = 0;\n","  float dim = 0;\n","\n","  //if (threadIdx.x == 0) printf(\"bitonic_sort_warp\\n\");\n","\n","  //phase 0 to log(128)-1 \n","  for(i=2; i<128 ;i*=2){ \n","    stage = 0;\n","\n","\n","    dim = i*2;\n","    u = ceil( (threadIdx.x+1) * (4/dim) ); //indice della sottosequenza simmetrica a cui il thread appartiene\n","    //printf(\"thread %d : u = %d \\n\", threadIdx.x, u);\n","\n","    index1 = (u - 1) * dim;\n","    index2 = index1 + dim - 1;\n","\n","    for(j = i/2; j > 0; j /= 2){ \n","      /*\n","      if (threadIdx.x == 0)\n","        printf(\"thread %d : phase = %d, stage = %d \\n\", threadIdx.x, phase, stage);\n","      */\n","      p = threadIdx.x - (u - 1) * (dim / 4); // posizione del thread nella sottosequenza simmetrica\n","      //printf(\"thread %d : p = %d \\n\", threadIdx.x, p);\n","\n","      //q Ã¨ l'offset usato poi per k_0 e k_1\n","      if (stage == 0) { // primo stage della fase\n","          q = p;\n","      }\n","      if (stage != 0 && stage != phase){ //nÃ© primo nÃ© ultimo stage della fase\n","          \n","          //int n = 2 ^ stage; // numero di minisequenze\n","          m = j; // numero di freccie rosse per minisequenza\n","          o = j * 2; //offset speciale tra minisequenza e l'altra\n","\n","          um = (int)(p / m); //indice della minisequenza a cui il thread appartiene\n","\n","          pm = p - um * m; //posizione del thread nella minisequenza\n","          q = pm + o * um;\n","      }\n","      if (stage == phase){ //ultimo stage della fase\n","          q = p * 2;\n","      }\n","      k_0 = index1 + q;\n","      k_1 = index2 - q; \n","\n","      k_0 = start + k_0;\n","      k_1 = start + k_1; \n","      \n","      //printf(\"thread %d : k_0 = %d, k_1 = %d \\n\", threadIdx.x, k_0, k_1);\n","\n","      //k_0 ? position of preceding element in each pair to form ascending order\n","      if(keyin[k_0] > keyin[k_0+j]) {\n","        int tmp = keyin[k_0];\n","        keyin[k_0] = keyin[k_0+j];\n","        keyin[k_0+j] = tmp;\n","      }\n","      //k1 ? position of preceding element in each pair to form descending order\n","      if(keyin[k_1] > keyin[k_1-j]){\n","        int tmp = keyin[k_1];\n","        keyin[k_1] = keyin[k_1-j];\n","        keyin[k_1-j] = tmp;\n","      }\n","\n","      stage++;\n","    }\n","    phase++;\n","  }\n","\n","  stage = 0;\n","  //special case for the last phase \n","  for(j=128/2; j>0; j/=2){\n","    \n","    dim = j * 2;\n","    if (dim < 4) dim = 4;\n","    u = ceil( (threadIdx.x+1) * (4/dim) ); //indice della sottosequenza simmetrica a cui il thread appartiene\n","\n","    //printf(\"thread %d : u = %d \\n\", threadIdx.x, u);\n","\n","    index1 = (u - 1) * dim;\n","    index2 = index1 + dim - 1;\n","\n","    p = threadIdx.x - (u - 1) * (dim / 4); // posizione del thread nella sottosequenza simmetrica\n","\n","    //q Ã¨ l'offset usato poi per k_0 e k_1\n","    \n","    q = p;\n","        \n","    k_0 = index1 + q;\n","    k_1 = index2 - q; \n","\n","    k_0 = start + k_0;\n","    k_1 = start + k_1;\n","\n","    /*\n","    if (threadIdx.x == 0)\n","        printf(\"thread %d : stage = %d, offset = %d \\n\", threadIdx.x, stage, j);\n","    printf(\"thread %d : k_0 = %d, k_1 = %d \\n\", threadIdx.x, k_0, k_1);\n","    */\n","      \n","    //k0 ? position of preceding element in the thread's first pair to form ascending order\n","    if(keyin[k_0] > keyin[k_0+j]){\n","        int tmp = keyin[k_0];\n","        keyin[k_0] = keyin[k_0 + j];\n","        keyin[k_0 + j] = tmp;\n","    }\n","\n","    //k1 ? position of preceding element in the thread's second pair to form ascending order\n","    if(keyin[k_1] < keyin[k_1 - j]){\n","        int tmp = keyin[k_1];\n","        keyin[k_1] = keyin[k_1 - j];\n","        keyin[k_1 - j] = tmp;\n","    }\n","\n","    stage++;\n","  }\n","}\n","\n","//STEP 2: Merge all the subsequences produced in step 1 until the parallelism is insufficient.\n","__global__ void bitonic_warp_merge(int * keyin, int * output, int offset){\n","  \n","  int j = 0;\n","  int stage = 0;\n","  int k_0 = 0;\n","  int u = 0, index1 = 0, p = 0;\n","  float dim = 0;\n","  \n","  __shared__ int buffer[T];\n","\n","  //unsigned int id = threadIdx.x + blockDim.x * blockIdx.x;\n","  unsigned int subseq = blockIdx.x; //in quale warp siamo\n","  unsigned int start = offset * subseq; //primo elemento della sottosequenza (A e B) da riordinare\n","\n","  //if (threadIdx.x == 0) printf(\"bitonic_warp_merge, offset = %d\\n\", offset);\n","\n","  int outIndex = start + threadIdx.x;\n","  int iA = start, iB = start + (offset / 2);\n","  int fA = start + (offset / 2), fB = start + offset;\n","  int tA = iA + threadIdx.x, tB = iB + threadIdx.x;\n","  bool compare;\n","\n","  /*\n","  if (threadIdx.x == 0){\n","      printf(\"block %d - thread %d: subseq = %d, offset = %d\\n\", blockIdx.x, threadIdx.x, subseq, offset);\n","      printf(\"block %d - thread %d: iA = %d, fA = %d, iB = %d, fB = %d \\n\", blockIdx.x, threadIdx.x, iA, iB, fA, fB);\n","  }*/\n","    \n","\n","  //printf(\"thread %d: tA = %d, tB = %d \\n\", threadIdx.x, tA, tB);\n","\n","  //prendo prima sequenza di A e la prima di B e le copio sul buffer\n","  buffer[T/2 - 1 - threadIdx.x] = keyin[tA];\n","  buffer[T/2 + threadIdx.x] = keyin[tB];\n","  tA += THREADS;\n","  tB += THREADS;\n","  \n","  //A[3] < B[3]\n","  compare = buffer[0] < buffer[T - 1]; //se true, al prossimo caricamento prendo i primi T/2 valori di A\n","\n","  int loops = 1;\n","  while(true)  {\n","\n","    /*\n","    if (threadIdx.x == 0){\n","      printf(\"loop = %d,\\nblock %d, thread %d, START of while: tA = %d, tB = %d \\n\", loops, blockIdx.x, threadIdx.x, tA, tB);\n","    }*/\n","    \n","\n","    stage = 0;\n","    //bitonic based merge sort\n","    for(j = T/2; j>0; j/=2){ \n","      \n","      dim = j * 2;\n","      if (dim < 2) dim = 2;\n","      u = ceil((threadIdx.x+1) * 2/dim); //indice della sottosequenza su cui il thread deve lavorare\n","\n","      //printf(\"thread %d : u = %d \\n\", threadIdx.x, u);\n","\n","      index1 = (u - 1) * dim; //primo indice della sottosequenza\n","\n","      p = threadIdx.x - (u - 1) * (dim / 2); // posizione del thread nella sottosequenza simmetrica\n","          \n","      k_0 = index1 + p;\n","\n","      /*\n","      if (threadIdx.x == 0){\n","        printf(\"block %d, thread %d : stage = %d, offset = %d \\n\", blockIdx.x, threadIdx.x, stage, j);\n","      }\n","      printf(\"block %d, thread %d : k_0 = %d \\n\", blockIdx.x, threadIdx.x, k_0);\n","      */\n","           \n","      //k0 ? position of preceding element in the thread's first pair to form ascending order\n","      if(buffer[k_0] > buffer[k_0 + j]){\n","          int tmp = buffer[k_0];\n","          buffer[k_0] = buffer[k_0 + j];\n","          buffer[k_0 + j] = tmp;\n","      }\n","\n","      stage++;\n","    }\n","    \n","    //carico i primi T/2 elementi di buffer sull'output\n","    output[outIndex] = buffer[threadIdx.x];\n","    outIndex += THREADS;\n","\n","    //se A e B finiscono elementi prima dell'algoritmo, prosegui solo con la sottosequenza rimanente\n","    if (tA > fA - 1 && tB < fB - 1)\n","      compare = false;\n","    if (tA < fA - 1 && tB > fB - 1)\n","      compare = true;\n","    if (tA > fA - 1 && tB > fB - 1){\n","        \n","      //carico gli ultimi T/2 elementi del buffer sull'output\n","      output[outIndex] = buffer[T/2 + threadIdx.x];\n","      \n","      break;\n","    }\n","      \n","    \n","    //usa il compare per caricare la prossima sottosequenza da A o B   \n","    if (compare){\n","      //carico T/2 elementi da A al buffer\n","      buffer[T/2 - 1 - threadIdx.x] = keyin[tA];\n","      tA += THREADS;\n","    } else {\n","      //carico T/2 elementi da B al buffer\n","      buffer[T/2 - 1 - threadIdx.x] = keyin[tB];\n","      tB += THREADS;\n","    }\n","\n","    if (compare){ //se avevo caricato dalla sequenza A, allora Amax Ã¨ il primo elemento del buffer e Bmax Ã¨ l'ultimo\n","        compare = buffer[0] < buffer[T - 1];\n","    } else { //altrimenti ho caricato B sul buffer, e Amax Ã¨ l'ultimo elemento, mentre Bmax Ã¨ il primo\n","        compare = buffer[0] > buffer[T - 1];\n","    }\n","\n","    loops++;\n","\n","    /*\n","    if (threadIdx.x == 0){\n","      printf(\"thread %d, END of while: tA = %d, tB = %d \\n\",threadIdx.x, tA, tB);\n","    }*/\n","  }\n","\n","}\n","\n","\n","/*******FUNZIONI DEL PROFESSORE*********/\n","\n","/*The parameter dir indicates the sorting direction, ASCENDING\n"," or DESCENDING; if (a[i] > a[j]) agrees with the direction,\n"," then a[i] and a[j] are interchanged.*/\n","void compAndSwap(int a[], int i, int j, int dir) {\n","\tif (dir == (a[i] > a[j])) {\n","\t\tint tmp = a[i];\n","\t\ta[i] = a[j];\n","\t\ta[j] = tmp;\n","\t}\n","}\n","\n","/*It recursively sorts a bitonic sequence in ascending order,\n"," if dir = 1, and in descending order otherwise (means dir=0).\n"," The sequence to be sorted starts at index position low,\n"," the parameter cnt is the number of elements to be sorted.*/\n","void bitonicMerge(int a[], int low, int cnt, int dir) {\n","\tif (cnt > 1) {\n","\t\tint k = cnt / 2;\n","\t\tfor (int i = low; i < low + k; i++)\n","\t\t\tcompAndSwap(a, i, i + k, dir);\n","\t\tbitonicMerge(a, low, k, dir);\n","\t\tbitonicMerge(a, low + k, k, dir);\n","\t}\n","}\n","\n","/* This function first produces a bitonic sequence by recursively\n"," sorting its two halves in opposite sorting orders, and then\n"," calls bitonicMerge to make them in the same order */\n","void bitonicSort(int a[], int low, int cnt, int dir) {\n","\tif (cnt > 1) {\n","\t\tint k = cnt / 2;\n","\n","\t\t// sort in ascending order since dir here is 1\n","\t\tbitonicSort(a, low, k, 1);\n","\n","\t\t// sort in descending order since dir here is 0\n","\t\tbitonicSort(a, low + k, k, 0);\n","\n","\t\t// Will merge wole sequence in ascending order\n","\t\t// since dir=1.\n","\t\tbitonicMerge(a, low, cnt, dir);\n","\t}\n","}\n","\n","/*\n"," ******************* MAIN *****************************************************************\n"," */\n","int main(void) {\n","\tcudaEvent_t start, stop;\n","\tcudaEventCreate(&start);\n","\tcudaEventCreate(&stop);\n","\n","\tint N = THREADS*4*BLOCKS;\n","\t// check\n","\tif (!(N && !(N & (N - 1)))) {\n","\t\tprintf(\"ERROR: N must be power of 2 (N = %d)\\n\", N);\n","\t\texit(1);\n","\t}\n","\tsize_t nBytes = N * sizeof(int);\n","\tint *a = (int*) malloc(nBytes);\n","\tint *b = (int*) malloc(nBytes);\n","\n","  srand ( time(NULL) );\n","\t// fill data\n","\tfor (int i = 0; i < N; ++i) {\n","\t\t//a[i] =  i%5; //rand() % 100; // / (float) RAND_MAX;\n","    a[i] = rand() % 100;\n","\t\tb[i] = a[i];\n","\t}\n","\n","\t// bitonic CPU\n","\tdouble cpu_time = seconds();\n","\n","  bitonicSort(b, 0, N, 1);   // 1 means sort in ascending order\n","\n","\tprintf(\"CPU elapsed time: %.5f (sec)\\n\", seconds()-cpu_time);\n","\n","\t// device mem copy\n","\tint *d_a, * d_b;\n","\tCHECK(cudaMalloc((void**) &d_a, nBytes));\n","  CHECK(cudaMalloc((void**) &d_b, nBytes));\n","\tCHECK(cudaMemcpy(d_a, a, nBytes, cudaMemcpyHostToDevice));\n","\n","\t// num of threads\n","\tdim3 blocks(BLOCKS, 1);   // Number of blocks\n","  dim3 threads(THREADS, 1); // Number of threads\n","\t\n","  /*\n","\tint j, k;\n","  // external loop on comparators of size k\n","  for (k = 2; k <= N; k <<= 1) {\n","    // internal loop for comparator internal stages\n","    for (j = k >> 1; j > 0; j = j >> 1)\n","      bitonic_sort_step<<<blocks, threads * 4>>>(d_a, j, k);\n","  }\n","  */\n","\n","  cudaDeviceProp deviceProp;\n","\tcudaGetDeviceProperties(&deviceProp, 0);\n","  int l = deviceProp.multiProcessorCount; //numero di streaming multiprocessor della GPU\n","  for (int k = 2; k < 1000; k *= 2){ \n","      if (k > l){ //TODO controllare se moltiplicando per 4 come suggerisce il paper migliora le prestazioni\n","          l = k;\n","          break;\n","      }\n","  }\n","  int s = l; \n","\n","  printf (\"\\nStreaming multiprocessors = %d\\n\", l);\n","\t\n","  \n","  // start computation\n","\tcudaEventRecord(start);\n","\n","  /*PRELIMINARY SPLITTER STEP3*********************************************************************/\n","  int *output = get_splitters (a, N, s);\n","  \n","  /*STEP 1: Divide the input sequence into equal-sized subsequences. *******************************************\n","  Each subsequence will be sorted by an independent warp using the bitonic network.*/\n","  bitonic_sort_warp<<<blocks, threads>>>(d_a);\n","\n","  /*STEP 2: Merge all the subsequences produced in step 1 until the parallelism is insufficient.*******************/\n","  //finchÃ¨ il parallelismo Ã¨ insufficiente, ovvero finchÃ¨ N / offset >= l\n","  //ad ogni warp merge si inverte input ed output\n","  bool isAfirst = true;\n","  blocks.x = BLOCKS / 2;   // Number of blocks\n","  l = 8; //TODO rimuovere, Ã¨ solo per testing!!!!!\n","  for(int offset = THREADS * 8; N / offset >= l ; offset *= 2){ \n","    if(isAfirst)\n","      bitonic_warp_merge<<<blocks, threads>>>(d_a, d_b, offset);\n","    else\n","      bitonic_warp_merge<<<blocks, threads>>>(d_b, d_a, offset);\n","    blocks.x = blocks.x / 2;\n","    \n","    isAfirst = !isAfirst;\n","  }\n","  //TODO ricordarsi che Afirst Ã¨ poi true se l'output finale Ã¨ in A, false se Ã¨ in B\n","  //TODO ricorarsi di resettare il numero di blocchi (warp)\n"," \n","  if(!isAfirst){\n","      int * temp = d_a;\n","      d_a = d_b;\n","      cudaFree(temp);\n","  }\n","  \n","  /*STEP 3: Split the large subsequences produced in step 2 into small ones that can be merged independently.*******************/\n","\n","  // recover data\n","  cudaMemcpy(a, d_a, nBytes, cudaMemcpyDeviceToHost);\n","\n","  int s_indexes[l][s];\n","  int temp_i;\n","  for (int i = 0; i < l; i++){ //per ogni riga \n","    temp_i = N / l * i;\n","    int splitCount = 0;\n","    s_indexes[i][splitCount] = temp_i; //inserisco l'indice per il primo segmento della riga\n","    splitCount++;\n","    //printf(\"indice %d di l = %d\\n\", i, s_indexes[i][0]);\n","    for(int j = temp_i; splitCount < s ; j++){ //calcolo gli indici dei rimanenti segmenti della riga\n","        if (output[splitCount] < a[j]){\n","            s_indexes[i][splitCount] = j;\n","            //printf(\"indice %d, %d di l, s = %d\\n\", i, splitCount, s_indexes[i][splitCount]);  \n","            splitCount++;  \n","        } \n","    }\n","  }\n","\n","  /****STEP 4: *************************************************************************************************/\n","  int *cpu_buffer; //buffer on cpu used to build the first s segment with -1 placeholders\n","  int *d_buffer, *d_buffer_temp;\n","  int s_length, global_index = 0;\n","  int global_s_lengths = 0;\n","\n","  int *a_output;\n","  a_output = (int*) malloc(nBytes);\n","\n","  for (int i = 0; i < s; i++){ //per ogni colonna\n","    //printf(\"\\n\\n---------------COLONNA---------------------- %d\\n\\n\", i);\n","    cpu_buffer = (int*) malloc(l * 128 * sizeof(int));\n","\n","    CHECK(cudaMalloc((void**) &d_buffer, l * 128 * sizeof(int)));\n","    CHECK(cudaMalloc((void**) &d_buffer_temp, l * 128 * sizeof(int)));\n","\n","    //copia dei valori dei segmenti s in un buffer\n","    for (int j = 0; j < l; j++){\n","      if (i + 1 >= s){\n","        if (j + 1 >= l)\n","          s_length = N - s_indexes[j][i]; //caso limite ultimo segmento\n","        else\n","          s_length = s_indexes[j + 1][0] - s_indexes[j][i]; //ultimo segmento della riga\n","      } else{\n","        s_length = s_indexes[j][i + 1] - s_indexes[j][i]; //calcoliamo la lunghezza del segmento s\n","      }\n","      \n","      //printf(\"segmento %d, %d: s_length = %d\\n\", j, i, s_length);\n","      global_s_lengths += s_length;\n","\n","      int s_index = s_indexes[j][i]; //troviamo la posizione del primo elemento del segmento s\n","      for (int k = 0 ; k < 128; k++){ //riempiamo il buffer con -1 e i valori del segmento s\n","        if (k < 128 - s_length){\n","          cpu_buffer[128 * j + k] = -1;\n","        } else {\n","          cpu_buffer[128 * j + k] = a[s_index];\n","          s_index++;\n","        }  \n","      }\n","    }\n","\n","    \n","    //print \n","    /*\n","    if (i == 0){\n","      printf(\"\\n**********STAMPA DEL BUFFER PRIMA DELLO STEP2 di s(x, %d)************\\n\\n\", i);\n","      for (int p = 0; p < l * 128; p++){ \n","        printf(\"cpu_buffer[%d] = %d\\n\", p, cpu_buffer[p]);\n","      }\n","    }*/\n","    \n","    CHECK(cudaMemcpy(d_buffer, cpu_buffer, l * 128 * sizeof(int), cudaMemcpyHostToDevice));\n","    free(cpu_buffer);\n","\n","    //fai step 2 su d_buffer (la colonna)\n","    blocks.x = l / 2;   // Number of blocks (warps)\n","    isAfirst = true;\n","    for(int offset = THREADS * 8; l * 128 / offset >= 1 ; offset *= 2){ \n","      //printf(\"\\nStep 2 presente!!!!\\n\\n\" );\n","      if(isAfirst)\n","        bitonic_warp_merge<<<blocks, threads>>>(d_buffer, d_buffer_temp, offset);\n","      else\n","        bitonic_warp_merge<<<blocks, threads>>>(d_buffer_temp, d_buffer, offset);\n","      blocks.x = blocks.x / 2;\n","      \n","      isAfirst = !isAfirst;\n","    }\n","    \n","    if(!isAfirst){\n","      CHECK(cudaMemcpy(cpu_buffer, d_buffer_temp, l * 128 * sizeof(int), cudaMemcpyDeviceToHost));\n","    } else {\n","      CHECK(cudaMemcpy(cpu_buffer, d_buffer, l * 128 * sizeof(int), cudaMemcpyDeviceToHost));\n","    }\n","    \n","    /*\n","    //printf(\"\\n**********STAMPA DEL BUFFER DOPO LO STEP 2 di s(x, %d)************\\n\\n\", i);\n","    int num_veri = 0;\n","    for (int p = 0; p < l * 128; p++){\n","        if (cpu_buffer[p] > -1){\n","          //if (i == 0) printf(\"cpu_buffer[%d] = %d\\n\", p, cpu_buffer[p]);\n","          num_veri++;\n","          global_index++;\n","        } \n","    }*/\n","   \n","    //printf(\"global_index nel for, colonna %d = %d\\n\", i, global_index);\n","    //printf(\"num_veri nel for, colonna %d = %d\\n\", i, num_veri);\n","    //printf(\"global_s_lengths nel for, colonna %d = %d\\n\", i, global_s_lengths);\n","    \n","    \n","    //salvo il buffer ordinato sull'output finale a rimuovendo i placeholder -1\n","    for(int z = 0; z < l * 128; z++){\n","      if (cpu_buffer[z] != -1){\n","        a_output[global_index] = cpu_buffer[z];\n","        printf(\"a[%d] = %d\\n\", global_index, a_output[global_index]);\n","        global_index++;\n","        \n","      } \n","    }\n","\n","    cudaFree(d_buffer); \n","    cudaFree(d_buffer_temp); \n","    free(cpu_buffer);\n","  }\n","\n","  //printf(\"\\n\\nglobal_index = %d\\n\", global_index);\n","  //printf(\"global_s_lengths = %d\\n\", global_s_lengths);\n","\n","\tcudaEventRecord(stop);\n","\tcudaEventSynchronize(stop);\n","\tfloat milliseconds = 0;\n","\tcudaEventElapsedTime(&milliseconds, start, stop);\n","\tprintf(\"GPU elapsed time: %.5f (sec)\\n\", milliseconds / 1000);\n","\n","\t// recover data\n","  //cudaMemcpy(a, d_a, nBytes, cudaMemcpyDeviceToHost);\n","\n","\n","\t// print & check\n","\tif (N < 100) {\n","\t\tprintf(\"GPU:\\n\");\n","\t\tfor (int i = 0; i < N; ++i){\n","      if(i % 128 == 0)\n","        printf(\"sottosequenza, indice = %d\\n\", i);\n","      printf(\"%d : %d\\n\", i, a[i]);\n","    }\n","\t\t\t\n","      /*\n","\t\tprintf(\"CPU:\\n\");\n","\t\tfor (int i = 0; i < N; ++i)\n","\t\t\tprintf(\"%d\\n\", b[i]);\n","      */\n","\t}\n","\telse {\n","    \n","\t\tfor (int i = 0; i < N; ++i) {\n","\t\t\tif (a_output[i] != b[i]) {\n","\t\t\t\tprintf(\"ERROR a[%d] != b[%d]  (a[i] = %d  -  b[i] = %d\\n\", i,i, a_output[i],b[i]);\n","\t\t\t\tbreak;\n","\t\t\t}\n","\t\t}\n","\t}\n","\n","  free(a);\n","  free(a_output);\n","\tcudaFree(d_a);\n","  cudaFree(d_b);\n","\texit(0);\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-wjq_suoB0z"},"source":["# compilatore\n","\n","!nvcc -arch=sm_37 WarpSort.cu -o WarpSort \n","! ./WarpSort\n"],"execution_count":null,"outputs":[]}]}